//lakukan ke master dan budak//
wget https://dlcdn.apache.org/spark/spark-3.5.4/spark-3.5.4-bin-hadoop3.tgz
tar -xvf spark-3.5.4-bin-hadoop3.tgz
mv spark-3.5.4-bin-hadoop3 Spark
sudo mv Spark /usr/share
sudo apt-get install scala
sudo nano .bashrc
  export SPARK_HOME=/usr/share/Spark
  export PATH=$PATH:/usr/share/Spark/bin
source .bashrc
cd /usr/share/Spark/conf
cp spark-env.sh.template spark-env.sh

java -version
ls -l /usr/lib/jvm

sudo nano spark-env.sh
  export SPARK_MASTER_HOST=master
  export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64

cp workers.template workers
sudo nano workers
budak1
budak2

//sampai sini//

cd /usr/share/Spark/sbin

./start-all.sh
jps (http://master:8080)

/etc/hosts



https://www.tutorialspoint.com/apache_spark/apache_spark_core_programming.htm
scala> 
val inputfile = sc.textFile("input.txt")
val counts = inputfile.flatMap(line => line.split(" ")).map(word => (word, 1)).reduceByKey(_+_);
counts.toDebugString
counts.cache()
counts.saveAsTextFile("output")
  buka cmd baru
cd output/
ls -1 
cat part-00000
cat part-00001 
http://localhost:4040
counts.unpersist()


# Import PySpark
from pyspark.sql import SparkSession

# Create a Spark session
spark = SparkSession.builder.master("local[*]").getOrCreate()
